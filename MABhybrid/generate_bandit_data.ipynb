{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrec_eval\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pytrec_eval\n",
      "  Building wheel for pytrec_eval (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp312-cp312-macosx_15_0_arm64.whl size=66768 sha256=f49910eee96add538f75117ad587097b7136269b0e18cb04430747532acd14c5\n",
      "  Stored in directory: /Users/eric/Library/Caches/pip/wheels/c6/4a/9e/e17f9ea004e1c221bd0ff384732285211c4917b790d598ea51\n",
      "Successfully built pytrec_eval\n",
      "Installing collected packages: pytrec_eval\n",
      "Successfully installed pytrec_eval-0.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "!pip install pyserini\n",
    "!pip install faiss-cpu\n",
    "!pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db710d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.search.faiss import FaissSearcher\n",
    "import pytrec_eval\n",
    "from pyserini.encode import TctColBertQueryEncoder\n",
    "# from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5f958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "OUTPUT_FILE = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "QUERIES_FILE = \"../MABhybrid/data/msmarco-train-queries.tsv\"\n",
    "QRELS_FILE = \"../MABhybrid/data/qrels.train.tsv\"\n",
    "SAMPLE_SIZE = 50000  # Number of queries to process\n",
    "TOP_K = 1000         # Depth of initial retrieval\n",
    "ARMS = [0.0, 0.25, 0.5, 0.75, 1.0] # Alpha values: 0.0=Dense, 1.0=Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c09783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(path, limit=None):\n",
    "    \"\"\"Loads queries from a TSV file (qid \\t text).\"\"\"\n",
    "    queries = []\n",
    "    print(f\"Loading queries from {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit and i >= limit: break\n",
    "            qid, text = line.strip().split('\\t')\n",
    "            queries.append((qid, text))\n",
    "    return queries\n",
    "\n",
    "def load_qrels(path):\n",
    "    \"\"\"Loads qrels into a dictionary: {qid: {docid: relevance}}.\"\"\"\n",
    "    qrels = {}\n",
    "    print(f\"Loading qrels from {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            qid, _, docid, rel = line.strip().split('\\t')\n",
    "            if qid not in qrels: qrels[qid] = {}\n",
    "            qrels[qid][docid] = int(rel)\n",
    "    return qrels\n",
    "\n",
    "def normalize_scores(hits):\n",
    "    \"\"\"Min-Max normalization of scores to [0, 1].\"\"\"\n",
    "    if not hits: return {}\n",
    "    scores = [h.score for h in hits]\n",
    "    min_s = min(scores)\n",
    "    max_s = max(scores)\n",
    "    \n",
    "    # Avoid division by zero if all scores are identical\n",
    "    if max_s == min_s: \n",
    "        return {h.docid: 1.0 for h in hits}\n",
    "        \n",
    "    # Return dictionary {docid: normalized_score}\n",
    "    norm_scores = {}\n",
    "    for h in hits:\n",
    "        norm_scores[h.docid] = (h.score - min_s) / (max_s - min_s)\n",
    "    return norm_scores\n",
    "\n",
    "# def calculate_ndcg(run_dict, qrels_dict, k=10):\n",
    "#     \"\"\"\n",
    "#     Calculates NDCG@k using pytrec_eval.\n",
    "#     run_dict: {docid: score}\n",
    "#     qrels_dict: {docid: relevance}\n",
    "#     \"\"\"\n",
    "#     if not qrels_dict: return 0.0\n",
    "    \n",
    "#     # pytrec_eval expects {qid: {docid: score}} structure\n",
    "#     evaluator = pytrec_eval.RelevanceEvaluator({'q': qrels_dict}, {'ndcg_cut'})\n",
    "#     results = evaluator.evaluate({'q': run_dict})\n",
    "#     return results['q'][f'ndcg_cut_{k}']\n",
    "\n",
    "# python\n",
    "def calculate_ndcg(qid, run_dict, qrels_dict, k=10):\n",
    "    \"\"\"\n",
    "    qid: str query id\n",
    "    run_dict: {docid: score}\n",
    "    qrels_dict: {docid: relevance}\n",
    "    \"\"\"\n",
    "    if not qrels_dict:\n",
    "        return 0.0\n",
    "\n",
    "    # Ensure native Python types (pytrec_eval C extension requires them)\n",
    "    run_cast = {str(d): float(s) for d, s in run_dict.items()}\n",
    "    qrels_cast = {str(d): int(r) for d, r in qrels_dict.items()}\n",
    "\n",
    "    metric = f'ndcg_cut_{k}'\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator({qid: qrels_cast}, {metric})\n",
    "    results = evaluator.evaluate({qid: run_cast})\n",
    "    return results[qid][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93949ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyserini.index.lucene import LuceneIndexReader\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Converts raw text queries into dense numerical vectors using Pyserini \n",
    "    for corpus statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self, index_path='msmarco-passage'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index_path (str): The name of the pre-built index (e.g., 'msmarco-passage')\n",
    "                              or a path to a local Lucene index.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing FeatureExtractor with index: {index_path}\")\n",
    "        # We must use IndexReader to access term statistics (df, cf), not LuceneSearcher\n",
    "        self.reader = LuceneIndexReader.from_prebuilt_index(index_path)\n",
    "        if not self.reader:\n",
    "             # Fallback for local paths if it's not a prebuilt index name\n",
    "            self.reader = LuceneIndexReader(index_path)\n",
    "            \n",
    "        # Get total number of documents (N) for IDF calculation\n",
    "        # .stats() returns a dict like {'documents': 8841823, 'non_empty_documents': ...}\n",
    "        self.N = self.reader.stats()['documents']\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"\n",
    "        Calculates Inverse Document Frequency (IDF) for a single term.\n",
    "        Formula: log( N / (df + 1) )\n",
    "        \"\"\"\n",
    "        # get_term_counts returns (df, cf). We only need df (Document Frequency).\n",
    "        # analyzer=None uses the default analyzer for the index (recommended).\n",
    "        try:\n",
    "            df, _ = self.reader.get_term_counts(term, analyzer=None)\n",
    "        except:\n",
    "            # Handle cases where term might cause Java encoding errors or not exist\n",
    "            df = 0\n",
    "            \n",
    "        # Avoid division by zero or log(0) issues\n",
    "        # Add 1 to df for smoothing\n",
    "        return np.log(self.N / (df + 1)) if (df + 1) > 0 else 0.0\n",
    "\n",
    "    def extract(self, query_text):\n",
    "        \"\"\"\n",
    "        Extracts the 5-dimensional feature vector for a query.\n",
    "        \n",
    "        Feature Definition:\n",
    "        1. Length: Number of tokens\n",
    "        2. Max IDF: Rarity of the rarest word (keyword specificity)\n",
    "        3. Avg IDF: Average rarity (information density)\n",
    "        4. Question Flag: 1.0 if starts with Wh-word, else 0.0\n",
    "        5. Bias: Constant 1.0 (Intercept)\n",
    "        \"\"\"\n",
    "        # Simple whitespace tokenization \n",
    "        # (For production, consider matching the Pyserini analyzer's tokenization)\n",
    "        tokens = query_text.lower().split()\n",
    "        length = len(tokens)\n",
    "        \n",
    "        if length == 0:\n",
    "            return np.array([0.0, 0.0, 0.0, 0.0, 1.0])\n",
    "        \n",
    "        # Compute IDFs\n",
    "        idfs = [self.get_idf(t) for t in tokens]\n",
    "        \n",
    "        max_idf = max(idfs) if idfs else 0.0\n",
    "        avg_idf = np.mean(idfs) if idfs else 0.0\n",
    "        \n",
    "        # Heuristic: Check for Wh-words to detect natural language questions\n",
    "        question_starters = {'who', 'what', 'where', 'when', 'why', 'how', 'which'}\n",
    "        is_question = 1.0 if tokens[0] in question_starters else 0.0\n",
    "        \n",
    "        # Construct and return the vector\n",
    "        # [Length, MaxIDF, AvgIDF, QuestionFlag, Bias]\n",
    "        return np.array([float(length), max_idf, avg_idf, is_question, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4935028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sparse Searcher (BM25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nov 28, 2025 8:49:06 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Pyserini Searchers\n",
    "print(\"Initializing Sparse Searcher (BM25)...\")\n",
    "# Automatically downloads the pre-built index\n",
    "sparse_searcher = LuceneSearcher.from_prebuilt_index('msmarco-passage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951cbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dense Searcher (TCT-ColBERT)...\n",
      "Attempting to initialize prebuilt index msmarco-passage-tct_colbert-hnsw.\n",
      "/Users/eric/.cache/pyserini/indexes/faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.6b7285a7f0163d1a547214396be20488 already exists, skipping download.\n",
      "Initializing msmarco-v1-passage.tct_colbert.hnsw...\n",
      "Dense Searcher initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Initializing Dense Searcher (TCT-ColBERT)...\")\n",
    "# Automatically downloads encoder and index\n",
    "encoder = TctColBertQueryEncoder('castorini/tct_colbert-msmarco')\n",
    "dense_searcher = FaissSearcher.from_prebuilt_index(\n",
    "    'msmarco-passage-tct_colbert-hnsw', \n",
    "    encoder\n",
    ")\n",
    "print(\"Dense Searcher initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca472bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FeatureExtractor with index: msmarco-passage\n",
      "Loading queries from ../MABhybrid/data/msmarco-train-queries.tsv...\n",
      "Loading qrels from ../MABhybrid/data/qrels.train.tsv...\n",
      "Found 502939 queries with judgments.\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup Feature Extractor\n",
    "# Reuse the sparse searcher's index reader for IDF stats\n",
    "feature_extractor = FeatureExtractor('msmarco-passage')\n",
    "\n",
    "# 3. Load Data\n",
    "all_queries = load_queries(QUERIES_FILE, limit=None) # Load all first, then sample\n",
    "all_qrels = load_qrels(QRELS_FILE)\n",
    "\n",
    "# Randomly sample queries that actually have qrels\n",
    "valid_queries = [q for q in all_queries if q[0] in all_qrels]\n",
    "print(f\"Found {len(valid_queries)} queries with judgments.\")\n",
    "\n",
    "# Shuffle and take 50k\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(valid_queries), min(SAMPLE_SIZE, len(valid_queries)), replace=False)\n",
    "sampled_queries = [valid_queries[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be4b0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyserini.index.lucene._base.LuceneIndexReader'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'analyze', 'compute_bm25_term_weight', 'compute_query_document_score', 'convert_collection_docid_to_internal_docid', 'convert_internal_docid_to_collection_docid', 'doc', 'doc_by_field', 'doc_contents', 'doc_raw', 'dump_documents_BM25', 'from_prebuilt_index', 'get_document_vector', 'get_postings_list', 'get_term_counts', 'get_term_positions', 'list_prebuilt_indexes', 'object', 'quantize_weights', 'reader', 'stats', 'terms', 'validate']\n",
      "['__doc__', 'compute_bm25_term_weight', 'compute_query_document_score', 'convert_collection_docid_to_internal_docid', 'convert_internal_docid_to_collection_docid', 'doc', 'doc_by_field', 'doc_contents', 'doc_raw', 'dump_documents_BM25', 'get_document_vector', 'get_term_counts', 'get_term_positions', 'terms']\n"
     ]
    }
   ],
   "source": [
    "# run in your notebook / REPL\n",
    "print(type(feature_extractor.reader))\n",
    "print(dir(feature_extractor.reader))\n",
    "print([m for m in dir(feature_extractor.reader) if 'term' in m.lower() or 'doc' in m.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16eb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing for 50000 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [7:39:27<00:00,  1.81it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Generated data saved to ../MABhybrid/data/bandit_data_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting processing for {len(sampled_queries)} queries...\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f_out:\n",
    "    for qid, text in tqdm(sampled_queries):\n",
    "        # A. Retrieve Sparse & Dense\n",
    "        try:\n",
    "            sparse_hits = sparse_searcher.search(text, k=TOP_K)\n",
    "            dense_hits = dense_searcher.search(text, k=TOP_K)\n",
    "        except Exception as e:\n",
    "            # Handle empty queries or encoding errors\n",
    "            continue\n",
    "\n",
    "        # B. Normalize Scores\n",
    "        # Map: {docid: norm_score}\n",
    "        sparse_dict = normalize_scores(sparse_hits)\n",
    "        dense_dict = normalize_scores(dense_hits)\n",
    "        \n",
    "        # Union of all retrieved docs\n",
    "        all_docs = set(sparse_dict.keys()) | set(dense_dict.keys())\n",
    "        \n",
    "        query_rewards = []\n",
    "        \n",
    "        # C. Fusion Loop (Calculate reward for each Arm)\n",
    "        for alpha in ARMS:\n",
    "            fused_scores = {}\n",
    "            for docid in all_docs:\n",
    "                s_score = sparse_dict.get(docid, 0.0)\n",
    "                d_score = dense_dict.get(docid, 0.0)\n",
    "                # Fusion Formula\n",
    "                score = alpha * s_score + (1.0 - alpha) * d_score\n",
    "                fused_scores[docid] = score\n",
    "            \n",
    "            # Sort top K for evaluation\n",
    "            # Note: pytrec_eval handles sorting, but we can limit size for speed\n",
    "            top_100 = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)[:100])\n",
    "            \n",
    "            # Calculate Reward (NDCG@10)\n",
    "            # reward = calculate_ndcg(top_100, all_qrels[qid], k=10)\n",
    "            reward = calculate_ndcg(qid, top_100, all_qrels[qid], k=10)\n",
    "            query_rewards.append(reward)\n",
    "\n",
    "        # D. Feature Extraction\n",
    "        features = feature_extractor.extract(text).tolist()\n",
    "\n",
    "        # Save\n",
    "        record = {\n",
    "            \"query_id\": qid,\n",
    "            \"text\": text,\n",
    "            \"features\": features,\n",
    "            \"rewards\": query_rewards,\n",
    "            \"optimal_arm\": int(np.argmax(query_rewards))\n",
    "        }\n",
    "        f_out.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Done! Generated data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5e1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
