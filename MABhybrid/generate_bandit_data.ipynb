{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db710d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "/Users/ativsc/python/ir-project/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.search.faiss import FaissSearcher\n",
    "import pytrec_eval\n",
    "from pyserini.encode import TctColBertQueryEncoder\n",
    "# from feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5f958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "OUTPUT_FILE = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "QUERIES_FILE = \"../MABhybrid/data/msmarco-train-queries.tsv\"\n",
    "QRELS_FILE = \"../MABhybrid/data/qrels.train.tsv\"\n",
    "SAMPLE_SIZE = 50000  # Number of queries to process\n",
    "TOP_K = 1000         # Depth of initial retrieval\n",
    "ARMS = [0.0, 0.25, 0.5, 0.75, 1.0] # Alpha values: 0.0=Dense, 1.0=Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c09783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(path, limit=None):\n",
    "    \"\"\"Loads queries from a TSV file (qid \\t text).\"\"\"\n",
    "    queries = []\n",
    "    print(f\"Loading queries from {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit and i >= limit: break\n",
    "            qid, text = line.strip().split('\\t')\n",
    "            queries.append((qid, text))\n",
    "    return queries\n",
    "\n",
    "def load_qrels(path):\n",
    "    \"\"\"Loads qrels into a dictionary: {qid: {docid: relevance}}.\"\"\"\n",
    "    qrels = {}\n",
    "    print(f\"Loading qrels from {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            qid, _, docid, rel = line.strip().split('\\t')\n",
    "            if qid not in qrels: qrels[qid] = {}\n",
    "            qrels[qid][docid] = int(rel)\n",
    "    return qrels\n",
    "\n",
    "def normalize_scores(hits):\n",
    "    \"\"\"Min-Max normalization of scores to [0, 1].\"\"\"\n",
    "    if not hits: return {}\n",
    "    scores = [h.score for h in hits]\n",
    "    min_s = min(scores)\n",
    "    max_s = max(scores)\n",
    "    \n",
    "    # Avoid division by zero if all scores are identical\n",
    "    if max_s == min_s: \n",
    "        return {h.docid: 1.0 for h in hits}\n",
    "        \n",
    "    # Return dictionary {docid: normalized_score}\n",
    "    norm_scores = {}\n",
    "    for h in hits:\n",
    "        norm_scores[h.docid] = (h.score - min_s) / (max_s - min_s)\n",
    "    return norm_scores\n",
    "\n",
    "# def calculate_ndcg(run_dict, qrels_dict, k=10):\n",
    "#     \"\"\"\n",
    "#     Calculates NDCG@k using pytrec_eval.\n",
    "#     run_dict: {docid: score}\n",
    "#     qrels_dict: {docid: relevance}\n",
    "#     \"\"\"\n",
    "#     if not qrels_dict: return 0.0\n",
    "    \n",
    "#     # pytrec_eval expects {qid: {docid: score}} structure\n",
    "#     evaluator = pytrec_eval.RelevanceEvaluator({'q': qrels_dict}, {'ndcg_cut'})\n",
    "#     results = evaluator.evaluate({'q': run_dict})\n",
    "#     return results['q'][f'ndcg_cut_{k}']\n",
    "\n",
    "# python\n",
    "def calculate_ndcg(qid, run_dict, qrels_dict, k=10):\n",
    "    \"\"\"\n",
    "    qid: str query id\n",
    "    run_dict: {docid: score}\n",
    "    qrels_dict: {docid: relevance}\n",
    "    \"\"\"\n",
    "    if not qrels_dict:\n",
    "        return 0.0\n",
    "\n",
    "    # Ensure native Python types (pytrec_eval C extension requires them)\n",
    "    run_cast = {str(d): float(s) for d, s in run_dict.items()}\n",
    "    qrels_cast = {str(d): int(r) for d, r in qrels_dict.items()}\n",
    "\n",
    "    metric = f'ndcg_cut_{k}'\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator({qid: qrels_cast}, {metric})\n",
    "    results = evaluator.evaluate({qid: run_cast})\n",
    "    return results[qid][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93949ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyserini.index.lucene import LuceneIndexReader\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Converts raw text queries into dense numerical vectors using Pyserini \n",
    "    for corpus statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self, index_path='msmarco-passage'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index_path (str): The name of the pre-built index (e.g., 'msmarco-passage')\n",
    "                              or a path to a local Lucene index.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing FeatureExtractor with index: {index_path}\")\n",
    "        # We must use IndexReader to access term statistics (df, cf), not LuceneSearcher\n",
    "        self.reader = LuceneIndexReader.from_prebuilt_index(index_path)\n",
    "        if not self.reader:\n",
    "             # Fallback for local paths if it's not a prebuilt index name\n",
    "            self.reader = LuceneIndexReader(index_path)\n",
    "            \n",
    "        # Get total number of documents (N) for IDF calculation\n",
    "        # .stats() returns a dict like {'documents': 8841823, 'non_empty_documents': ...}\n",
    "        self.N = self.reader.stats()['documents']\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"\n",
    "        Calculates Inverse Document Frequency (IDF) for a single term.\n",
    "        Formula: log( N / (df + 1) )\n",
    "        \"\"\"\n",
    "        # get_term_counts returns (df, cf). We only need df (Document Frequency).\n",
    "        # analyzer=None uses the default analyzer for the index (recommended).\n",
    "        try:\n",
    "            df, _ = self.reader.get_term_counts(term, analyzer=None)\n",
    "        except:\n",
    "            # Handle cases where term might cause Java encoding errors or not exist\n",
    "            df = 0\n",
    "            \n",
    "        # Avoid division by zero or log(0) issues\n",
    "        # Add 1 to df for smoothing\n",
    "        return np.log(self.N / (df + 1)) if (df + 1) > 0 else 0.0\n",
    "\n",
    "    def extract(self, query_text):\n",
    "        \"\"\"\n",
    "        Extracts the 5-dimensional feature vector for a query.\n",
    "        \n",
    "        Feature Definition:\n",
    "        1. Length: Number of tokens\n",
    "        2. Max IDF: Rarity of the rarest word (keyword specificity)\n",
    "        3. Avg IDF: Average rarity (information density)\n",
    "        4. Question Flag: 1.0 if starts with Wh-word, else 0.0\n",
    "        5. Bias: Constant 1.0 (Intercept)\n",
    "        \"\"\"\n",
    "        # Simple whitespace tokenization \n",
    "        # (For production, consider matching the Pyserini analyzer's tokenization)\n",
    "        tokens = query_text.lower().split()\n",
    "        length = len(tokens)\n",
    "        \n",
    "        if length == 0:\n",
    "            return np.array([0.0, 0.0, 0.0, 0.0, 1.0])\n",
    "        \n",
    "        # Compute IDFs\n",
    "        idfs = [self.get_idf(t) for t in tokens]\n",
    "        \n",
    "        max_idf = max(idfs) if idfs else 0.0\n",
    "        avg_idf = np.mean(idfs) if idfs else 0.0\n",
    "        \n",
    "        # Heuristic: Check for Wh-words to detect natural language questions\n",
    "        question_starters = {'who', 'what', 'where', 'when', 'why', 'how', 'which'}\n",
    "        is_question = 1.0 if tokens[0] in question_starters else 0.0\n",
    "        \n",
    "        # Construct and return the vector\n",
    "        # [Length, MaxIDF, AvgIDF, QuestionFlag, Bias]\n",
    "        return np.array([float(length), max_idf, avg_idf, is_question, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4935028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sparse Searcher (BM25)...\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Pyserini Searchers\n",
    "print(\"Initializing Sparse Searcher (BM25)...\")\n",
    "# Automatically downloads the pre-built index\n",
    "sparse_searcher = LuceneSearcher.from_prebuilt_index('msmarco-passage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9efc5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<io.anserini.search.ScoredDoc at 0x3257cc5a0 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a7610e2 at 0x400a10a30>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cc780 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a7610ea at 0x400a10b50>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cc8c0 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a7610f2 at 0x400a10990>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cc910 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a7610fa at 0x400a108d0>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cc960 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a761102 at 0x400a10870>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cc9b0 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a76110a at 0x400a109f0>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cca00 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a76111a at 0x400a10af0>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cd1d0 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a761122 at 0x400a10ab0>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cd220 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a76112a at 0x400a109d0>>,\n",
       " <io.anserini.search.ScoredDoc at 0x3257cd270 jclass=io/anserini/search/ScoredDoc jself=<LocalRef obj=0x30a761132 at 0x400a107f0>>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_searcher.search(\"test\")  # Warm-up call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951cbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dense Searcher (TCT-ColBERT)...\n",
      "Attempting to initialize prebuilt index msmarco-passage-tct_colbert-hnsw.\n",
      "/Users/ativsc/.cache/pyserini/indexes/faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.6b7285a7f0163d1a547214396be20488 already exists, skipping download.\n",
      "Initializing msmarco-v1-passage.tct_colbert.hnsw...\n",
      "Dense Searcher initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Initializing Dense Searcher (TCT-ColBERT)...\")\n",
    "# Automatically downloads encoder and index\n",
    "encoder = TctColBertQueryEncoder('castorini/tct_colbert-msmarco')\n",
    "dense_searcher = FaissSearcher.from_prebuilt_index(\n",
    "    'msmarco-passage-tct_colbert-hnsw', \n",
    "    encoder\n",
    ")\n",
    "print(\"Dense Searcher initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca472bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FeatureExtractor with index: msmarco-passage\n",
      "Loading queries from ../MABhybrid/data/msmarco-train-queries.tsv...\n",
      "Loading qrels from ../MABhybrid/data/qrels.train.tsv...\n",
      "Found 502939 queries with judgments.\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup Feature Extractor\n",
    "# Reuse the sparse searcher's index reader for IDF stats\n",
    "feature_extractor = FeatureExtractor('msmarco-passage')\n",
    "\n",
    "# 3. Load Data\n",
    "all_queries = load_queries(QUERIES_FILE, limit=None) # Load all first, then sample\n",
    "all_qrels = load_qrels(QRELS_FILE)\n",
    "\n",
    "# Randomly sample queries that actually have qrels\n",
    "valid_queries = [q for q in all_queries if q[0] in all_qrels]\n",
    "print(f\"Found {len(valid_queries)} queries with judgments.\")\n",
    "\n",
    "# Shuffle and take 50k\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(valid_queries), min(SAMPLE_SIZE, len(valid_queries)), replace=False)\n",
    "sampled_queries = [valid_queries[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4b0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyserini.index.lucene._base.LuceneIndexReader'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'analyze', 'compute_bm25_term_weight', 'compute_query_document_score', 'convert_collection_docid_to_internal_docid', 'convert_internal_docid_to_collection_docid', 'doc', 'doc_by_field', 'doc_contents', 'doc_raw', 'dump_documents_BM25', 'from_prebuilt_index', 'get_document_vector', 'get_postings_list', 'get_term_counts', 'get_term_positions', 'list_prebuilt_indexes', 'object', 'quantize_weights', 'reader', 'stats', 'terms', 'validate']\n",
      "['__doc__', 'compute_bm25_term_weight', 'compute_query_document_score', 'convert_collection_docid_to_internal_docid', 'convert_internal_docid_to_collection_docid', 'doc', 'doc_by_field', 'doc_contents', 'doc_raw', 'dump_documents_BM25', 'get_document_vector', 'get_term_counts', 'get_term_positions', 'terms']\n"
     ]
    }
   ],
   "source": [
    "# run in your notebook / REPL\n",
    "print(type(feature_extractor.reader))\n",
    "print(dir(feature_extractor.reader))\n",
    "print([m for m in dir(feature_extractor.reader) if 'term' in m.lower() or 'doc' in m.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16eb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing for 50000 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/50000 [10:02<83:30:51,  6.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/rb/mnvmmctx0vs__kg44cn7wcmm0000gn/T/ipykernel_32064/1621086807.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# A. Retrieve Sparse & Dense\u001b[39;00m\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[32m      8\u001b[39m         sparse_hits = sparse_searcher.search(text, k=TOP_K)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         dense_hits = dense_searcher.search(text, k=TOP_K)\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[32m     11\u001b[39m         \u001b[38;5;66;03m#     # Handle empty queries or encoding errors\u001b[39;00m\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m#     print(f\"Error retrieving for query {qid}: {e}\", file=sys.stderr)\u001b[39;00m\n",
      "\u001b[32m~/python/ir-project/.conda/lib/python3.12/site-packages/pyserini/search/faiss/_searcher.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, query, k, threads, remove_dups, return_vector)\u001b[39m\n\u001b[32m    143\u001b[39m             indexes = indexes.flat\n\u001b[32m    144\u001b[39m             return emb_q, [PrfDenseSearchResult(self.docids[idx], score, vector)\n\u001b[32m    145\u001b[39m                            \u001b[38;5;28;01mfor\u001b[39;00m score, idx, vector \u001b[38;5;28;01min\u001b[39;00m zip(distances, indexes, vectors) \u001b[38;5;28;01mif\u001b[39;00m idx != -\u001b[32m1\u001b[39m]\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m             distances, indexes = self.index.search(emb_q, k)\n\u001b[32m    148\u001b[39m             distances = distances.flat\n\u001b[32m    149\u001b[39m             indexes = indexes.flat\n\u001b[32m    150\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m remove_dups:\n",
      "\u001b[32m~/python/ir-project/.conda/lib/python3.12/site-packages/faiss/class_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x, k, params, D, I, numeric_type)\u001b[39m\n\u001b[32m    394\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    395\u001b[39m             \u001b[38;5;28;01massert\u001b[39;00m I.shape == (n, k)\n\u001b[32m    396\u001b[39m \n\u001b[32m    397\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m numeric_type == faiss.Float32:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             self.search_c(n, swig_ptr(x), k, swig_ptr(D), swig_ptr(I), params)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    400\u001b[39m             self.search_ex(n, swig_ptr(x), numeric_type, k, swig_ptr(D), swig_ptr(I), params)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m D, I\n",
      "\u001b[32m~/python/ir-project/.conda/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, n, x, k, distances, labels, params)\u001b[39m\n\u001b[32m   7380\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m search(self, n, x, k, distances, labels, params=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   7381\u001b[39m         \u001b[33mr\"\"\"entry point for search\"\"\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m7382\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss.IndexHNSW_search(self, n, x, k, distances, labels, params)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Starting processing for {len(sampled_queries)} queries...\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f_out:\n",
    "    for qid, text in tqdm(sampled_queries):\n",
    "        # A. Retrieve Sparse & Dense\n",
    "        # print(text)\n",
    "        # try:\n",
    "        sparse_hits = sparse_searcher.search(text, k=TOP_K)\n",
    "        dense_hits = dense_searcher.search(text, k=TOP_K)\n",
    "        # except Exception as e:\n",
    "        #     # Handle empty queries or encoding errors\n",
    "        #     print(f\"Error retrieving for query {qid}: {e}\", file=sys.stderr)\n",
    "        #     continue\n",
    "\n",
    "        # B. Normalize Scores\n",
    "        # Map: {docid: norm_score}\n",
    "        sparse_dict = normalize_scores(sparse_hits)\n",
    "        dense_dict = normalize_scores(dense_hits)\n",
    "        \n",
    "        # Union of all retrieved docs\n",
    "        all_docs = set(sparse_dict.keys()) | set(dense_dict.keys())\n",
    "        \n",
    "        query_rewards = []\n",
    "        \n",
    "        # C. Fusion Loop (Calculate reward for each Arm)\n",
    "        for alpha in ARMS:\n",
    "            fused_scores = {}\n",
    "            for docid in all_docs:\n",
    "                s_score = sparse_dict.get(docid, 0.0)\n",
    "                d_score = dense_dict.get(docid, 0.0)\n",
    "                # Fusion Formula\n",
    "                score = alpha * s_score + (1.0 - alpha) * d_score\n",
    "                fused_scores[docid] = score\n",
    "            \n",
    "            # Sort top K for evaluation\n",
    "            # Note: pytrec_eval handles sorting, but we can limit size for speed\n",
    "            top_100 = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)[:100])\n",
    "            \n",
    "            # Calculate Reward (NDCG@10)\n",
    "            # reward = calculate_ndcg(top_100, all_qrels[qid], k=10)\n",
    "            reward = calculate_ndcg(qid, top_100, all_qrels[qid], k=10)\n",
    "            query_rewards.append(reward)\n",
    "\n",
    "        # D. Feature Extraction\n",
    "        features = feature_extractor.extract(text).tolist()\n",
    "\n",
    "        # Save\n",
    "        record = {\n",
    "            \"query_id\": qid,\n",
    "            \"text\": text,\n",
    "            \"features\": features,\n",
    "            \"rewards\": query_rewards,\n",
    "            \"optimal_arm\": int(np.argmax(query_rewards))\n",
    "        }\n",
    "        f_out.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Done! Generated data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5e1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
