{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94237fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MSMARCO minmax\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "HISTORY_FILE = \"../MABhybrid/data/linucb_training_history.jsonl\"\n",
    "DATA_FILE = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "OUTPUT_DIR = \"../MABhybrid/fig\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads history and enriches it with baseline rewards from ground truth.\"\"\"\n",
    "    if not os.path.exists(HISTORY_FILE) or not os.path.exists(DATA_FILE):\n",
    "        print(\"Error: Missing history or data file.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Loading training history...\")\n",
    "    history = []\n",
    "    with open(HISTORY_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            history.append(json.loads(line))\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "    print(\"Loading ground truth data for baselines...\")\n",
    "    ground_truth = []\n",
    "    with open(DATA_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            ground_truth.append(json.loads(line))\n",
    "    \n",
    "    # Calculate Baseline Rewards\n",
    "    # We assume the training loop iterated through ground_truth sequentially\n",
    "    n_data = len(ground_truth)\n",
    "    \n",
    "    static_rewards = []\n",
    "    random_rewards = []\n",
    "    optimal_rewards = []\n",
    "\n",
    "    for step in df['step']:\n",
    "        # Map step to index in the original data file (handling wrap-around if any)\n",
    "        idx = step % n_data\n",
    "        entry = ground_truth[idx]\n",
    "        \n",
    "        # Static Baseline: Always choose Arm 2 (Index 2 corresponds to alpha=0.5)\n",
    "        static_rewards.append(entry['rewards'][2])\n",
    "        \n",
    "        # Random Baseline: Expected value is the mean of all arms\n",
    "        random_rewards.append(np.mean(entry['rewards']))\n",
    "        \n",
    "        # Optimal Reward (for calculating regret)\n",
    "        optimal_rewards.append(max(entry['rewards']))\n",
    "\n",
    "    df['static_reward'] = static_rewards\n",
    "    df['random_reward'] = random_rewards\n",
    "    df['optimal_reward'] = optimal_rewards\n",
    "    \n",
    "    # Calculate Regrets\n",
    "    df['static_regret'] = df['optimal_reward'] - df['static_reward']\n",
    "    df['random_regret'] = df['optimal_reward'] - df['random_reward']\n",
    "    # 'regret' column already exists for the agent in the history file\n",
    "    \n",
    "    return df, ground_truth\n",
    "\n",
    "def plot_comparative_cumulative_reward(df):\n",
    "    \"\"\"\n",
    "    Plots Cumulative Mean Reward: Agent vs Static vs Random\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate expanding means (cumulative average)\n",
    "    df['agent_cum_mean'] = df['reward'].expanding().mean()\n",
    "    df['static_cum_mean'] = df['static_reward'].expanding().mean()\n",
    "    df['random_cum_mean'] = df['random_reward'].expanding().mean()\n",
    "    \n",
    "    plt.plot(df['step'], df['agent_cum_mean'], label='LinUCB Agent', color='blue', linewidth=2)\n",
    "    plt.plot(df['step'], df['static_cum_mean'], label='Static (α=0.5)', color='green', linestyle='--')\n",
    "    plt.plot(df['step'], df['random_cum_mean'], label='Random', color='gray', linestyle=':')\n",
    "    \n",
    "    plt.title('Performance Comparison: Cumulative Mean Reward (NDCG@10)')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Average Reward (NDCG)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, 'comparative_reward.png')\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Saved Comparative Reward plot to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_comparative_cumulative_regret(df):\n",
    "    \"\"\"\n",
    "    Plots Cumulative Regret: Agent vs Static vs Random\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(df['step'], df['regret'].cumsum(), label='LinUCB Agent', color='red', linewidth=2)\n",
    "    plt.plot(df['step'], df['static_regret'].cumsum(), label='Static (α=0.5)', color='green', linestyle='--')\n",
    "    plt.plot(df['step'], df['random_regret'].cumsum(), label='Random', color='gray', linestyle=':')\n",
    "    \n",
    "    plt.title('Performance Comparison: Cumulative Regret')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Cumulative Regret (Lost NDCG)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, 'comparative_regret.png')\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Saved Comparative Regret plot to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_arm_distribution(df):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    arm_counts = df['chosen_arm'].value_counts().sort_index()\n",
    "    arms = arm_counts.index.tolist()\n",
    "    counts = arm_counts.values.tolist()\n",
    "    alpha_map = {0: '0.0 (Dense)', 1: '0.25', 2: '0.5 (Hybrid)', 3: '0.75', 4: '1.0 (Sparse)'}\n",
    "    labels = [alpha_map.get(a, str(a)) for a in arms]\n",
    "    \n",
    "    plt.bar(arms, counts, color='purple', alpha=0.7)\n",
    "    plt.xticks(arms, labels, rotation=45)\n",
    "    plt.title('Agent Choice Distribution')\n",
    "    plt.xlabel('Arm (Alpha Value)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'arm_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_importance(history_df, ground_truth_data):\n",
    "    \"\"\"\n",
    "    Reconstructs the agent to analyze feature weights.\n",
    "    \"\"\"\n",
    "    # Import locally to avoid issues if MABhybrid isn't in pythonpath\n",
    "    # sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'MABhybrid'))\n",
    "    # try:\n",
    "    #     from linucb import LinUCBAgent\n",
    "    # except ImportError:\n",
    "    #     print(\"Could not import LinUCBAgent for feature analysis.\")\n",
    "    #     return\n",
    "\n",
    "    print(\"\\n--- Reconstructing Agent for Feature Analysis ---\")\n",
    "    # 5 features: Length, MaxIDF, AvgIDF, QFlag, Bias\n",
    "    agent = LinUCBAgent(n_arms=5, n_features=5, alpha=0.1)\n",
    "    \n",
    "    # Replay history to update agent\n",
    "    n_data = len(ground_truth_data)\n",
    "    \n",
    "    # Limit reconstruction to first 50k steps or full length to save time if large\n",
    "    steps_to_replay = min(len(history_df), 50000) \n",
    "    \n",
    "    for i in range(steps_to_replay):\n",
    "        row = history_df.iloc[i]\n",
    "        step_idx = int(row['step']) % n_data\n",
    "        \n",
    "        # Get context from ground truth\n",
    "        features = np.array(ground_truth_data[step_idx]['features'])\n",
    "        chosen_arm = int(row['chosen_arm'])\n",
    "        reward = float(row['reward'])\n",
    "        \n",
    "        agent.update(chosen_arm, features, reward)\n",
    "\n",
    "    feature_names = ['Length', 'MaxIDF', 'AvgIDF', 'QFlag', 'Bias']\n",
    "    \n",
    "    # Print weights for Sparse (Arm 4) vs Dense (Arm 0)\n",
    "    print(\"\\n[Interpretability Result] Learned Feature Weights:\")\n",
    "    \n",
    "    # Helper to print vector\n",
    "    def print_arm_weights(arm_idx, name):\n",
    "        theta = np.linalg.inv(agent.A[arm_idx]) @ agent.b[arm_idx]\n",
    "        print(f\"\\nArm {arm_idx} ({name}):\")\n",
    "        for f, w in zip(feature_names, theta):\n",
    "            print(f\"  {f:10s}: {w: .4f}\")\n",
    "\n",
    "    print_arm_weights(0, \"Dense Retrieval\")\n",
    "    print_arm_weights(4, \"Sparse Retrieval\")\n",
    "\n",
    "def main():\n",
    "    result = load_data()\n",
    "    if result is None: return\n",
    "    df, ground_truth = result\n",
    "    \n",
    "    print(f\"Loaded {len(df)} training steps.\")\n",
    "    \n",
    "    plot_comparative_cumulative_reward(df)\n",
    "    plot_comparative_cumulative_regret(df)\n",
    "    plot_arm_distribution(df)\n",
    "    analyze_feature_importance(df, ground_truth)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Check {OUTPUT_DIR} for plots.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d9c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
