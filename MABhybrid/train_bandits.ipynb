{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc51a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a864f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAB utils\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "# from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import re\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import pandas as pd\n",
    "import ast  \n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, arms, dataset, args=None, preding=False):\n",
    "        self.arms = arms\n",
    "        self.dataset = dataset\n",
    "        self.preding = preding\n",
    "        self.index = -1\n",
    "        self.alpha_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        self.skip_dataset = []\n",
    "        self._update_state()\n",
    "        \n",
    "\n",
    "    def _update_state(self):\n",
    "        self.index += 1\n",
    "        if self.index >= len(self.dataset):\n",
    "            self.index = 0\n",
    "        \n",
    "        while self.dataset[self.index]['dataset_name'] in self.skip_dataset and not self.preding:\n",
    "            self.index += 1\n",
    "            if self.index >= len(self.dataset):\n",
    "                self.index = 0\n",
    "\n",
    "        self.state = self.dataset[self.index]['text']\n",
    "        \n",
    "        # self.state = np.random.randint(0, self.arms)\n",
    "    def _index_to_arm(self,index):\n",
    "        if type(index) == np.ndarray:\n",
    "            assert len(index) == 1\n",
    "            index = index[0]\n",
    "        return self.alpha_values[int(index)]\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "        # return self.state\n",
    "\n",
    "    def _get_reward(self, arm):\n",
    "        \"\"\"\n",
    "        Returns the pre-computed reward for the selected arm.\n",
    "        arm (int): Index 0-4 corresponding to alpha values [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        \"\"\"\n",
    "        query_data = self.dataset[self.index]\n",
    "        rewards = query_data.get(\"rewards\", [0.0] * 5)\n",
    "        \n",
    "        # Validate arm index\n",
    "        if arm < 0 or arm >= len(rewards):\n",
    "            print(f\"Warning: Arm {arm} out of range for rewards list of length {len(rewards)}\")\n",
    "            return 0.0\n",
    "        \n",
    "        return float(rewards[int(arm)])\n",
    "                               \n",
    "\n",
    "    def _get_recall(self,arm):\n",
    "        raise NotImplementedError\n",
    "        method = self._index_to_arm(arm)\n",
    "        return self.dataset[self.index][method+'_eval']['recall']\n",
    "\n",
    "    def choose_arm(self, arm):\n",
    "        reward = self._get_reward(arm)\n",
    "        # recall = self._get_recall(arm)\n",
    "        self._update_state()\n",
    "        return reward\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1250827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinUCBAgent:\n",
    "    \"\"\"\n",
    "    Disjoint Linear Upper Confidence Bound (LinUCB) Agent.\n",
    "    \n",
    "    References:\n",
    "        Li et al., \"A Contextual-Bandit Approach to Personalized News Article Recommendation\", WWW 2010.\n",
    "        (Algorithm 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_features, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms (int): Number of distinct actions (fusion weights).\n",
    "            n_features (int): Dimension of the context vector.\n",
    "            alpha (float): Exploration hyperparameter. Higher alpha = more exploration.\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize disjoint matrices for each arm\n",
    "        # A: Covariance matrix (d x d), initialized to Identity for Ridge Regularization\n",
    "        # b: Reward-weighted feature vector (d x 1), initialized to zeros\n",
    "        self.A = [np.identity(n_features) for _ in range(n_arms)]\n",
    "        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self, context_vector):\n",
    "        \"\"\"\n",
    "        Selects an arm based on the Upper Confidence Bound (UCB) of the estimated reward.\n",
    "        \n",
    "        Args:\n",
    "            context_vector (np.array): Shape (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of the selected arm.\n",
    "        \"\"\"\n",
    "        p = np.zeros(self.n_arms)\n",
    "        \n",
    "        for a in range(self.n_arms):\n",
    "            # 1. Compute the inverse of A (Ridge Regression covariance)\n",
    "            # In production, use np.linalg.solve or update inverse iteratively for speed\n",
    "            A_inv = np.linalg.inv(self.A[a])\n",
    "            \n",
    "            # 2. Compute the estimated coefficient (theta)\n",
    "            # theta = A^-1 * b\n",
    "            theta = A_inv @ self.b[a]\n",
    "            \n",
    "            # 3. Calculate the standard deviation (uncertainty width)\n",
    "            # std = sqrt(x.T * A^-1 * x)\n",
    "            std_dev = np.sqrt(context_vector.T @ A_inv @ context_vector)\n",
    "            \n",
    "            # 4. Calculate UCB\n",
    "            # Prediction + Exploration Bonus\n",
    "            p[a] = theta @ context_vector + self.alpha * std_dev\n",
    "            \n",
    "        # Tie-breaking: randomly choose among max if multiple arms share the same score\n",
    "        # (np.argmax usually takes the first occurrence, which is fine here)\n",
    "        return np.argmax(p)\n",
    "\n",
    "    def update(self, arm, context_vector, reward):\n",
    "        \"\"\"\n",
    "        Updates the internal matrices A and b for the specific arm that was chosen.\n",
    "        \n",
    "        Args:\n",
    "            arm (int): The arm index that was selected.\n",
    "            context_vector (np.array): The feature vector observed.\n",
    "            reward (float): The actual reward (NDCG) received.\n",
    "        \"\"\"\n",
    "        # Outer product of context vector (d x d)\n",
    "        self.A[arm] += np.outer(context_vector, context_vector)\n",
    "        \n",
    "        # Update bias vector\n",
    "        self.b[arm] += reward * context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71a748fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x, y, sigma=1.0):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return np.exp(-np.linalg.norm(x - y) ** 2 / (2.0 * sigma ** 2))\n",
    "\n",
    "class FastKernelUCBAgent:\n",
    "    \"\"\"\n",
    "    NumPy-only Fast Kernel UCB agent (incrementally updates kernel inverse).\n",
    "    Compatible with existing train_agent usage: select_arm(context: np.array) -> int,\n",
    "    update(arm: int, x_new: np.array, reward: float).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, kernel_fn=rbf_kernel, alpha=1.0, lambda_reg=1.0, device='cpu', sigma=1.0):\n",
    "        self.n_arms = n_arms\n",
    "        self.kernel_fn = kernel_fn\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = float(lambda_reg)\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # Per-arm stored data\n",
    "        self.X = [[] for _ in range(n_arms)]      # list of lists of np.arrays\n",
    "        self.y = [[] for _ in range(n_arms)]      # list of lists of floats\n",
    "        self.K_inv = [None for _ in range(n_arms)]  # list of np.ndarray or None\n",
    "\n",
    "    def _kernel_vector(self, x, X_list):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return np.array([self.kernel_fn(x, xi, sigma=self.sigma) for xi in X_list], dtype=float)\n",
    "\n",
    "    def select_arm(self, context_vector):\n",
    "        x = np.asarray(context_vector, dtype=float)\n",
    "        p = np.full((self.n_arms,), -np.inf, dtype=float)\n",
    "\n",
    "        for arm in range(self.n_arms):\n",
    "            if len(self.X[arm]) == 0:\n",
    "                p[arm] = np.inf  # force exploration of unseen arms\n",
    "                continue\n",
    "\n",
    "            X_arm = self.X[arm]\n",
    "            y_arm = np.array(self.y[arm], dtype=float).flatten()  # shape (n_old,)\n",
    "            K_inv = self.K_inv[arm]  # shape (n_old, n_old)\n",
    "\n",
    "            k_vec = self._kernel_vector(x, X_arm)  # shape (n_old,)\n",
    "\n",
    "            mean = float(k_vec.dot(K_inv.dot(y_arm)))\n",
    "            k_xx = float(self.kernel_fn(x, x, sigma=self.sigma))\n",
    "\n",
    "            var = k_xx - float(k_vec.dot(K_inv.dot(k_vec)))\n",
    "            var = max(var, 1e-9)\n",
    "            std = np.sqrt(var)\n",
    "\n",
    "            p[arm] = mean + self.alpha * std\n",
    "\n",
    "        return int(np.argmax(p))\n",
    "\n",
    "    def update(self, arm, x_new, reward):\n",
    "        x = np.asarray(x_new, dtype=float)\n",
    "        X_arm = self.X[arm]\n",
    "        K_inv = self.K_inv[arm]\n",
    "\n",
    "        # Append new sample and reward\n",
    "        X_arm.append(x)\n",
    "        self.y[arm].append(float(reward))\n",
    "\n",
    "        if len(X_arm) == 1:\n",
    "            k_xx = float(self.kernel_fn(x, x, sigma=self.sigma))\n",
    "            self.K_inv[arm] = np.array([[1.0 / (k_xx + self.lambda_reg)]], dtype=float)\n",
    "            return\n",
    "\n",
    "        # Compute quantities for block matrix inverse update\n",
    "        k_vec = self._kernel_vector(x, X_arm[:-1])  # shape (n_old,)\n",
    "        k_xx = float(self.kernel_fn(x, x, sigma=self.sigma))\n",
    "\n",
    "        # Ensure K_inv is np.ndarray\n",
    "        K_inv = np.asarray(K_inv, dtype=float)\n",
    "        term1 = K_inv.dot(k_vec)  # shape (n_old,)\n",
    "        c = float(k_xx + self.lambda_reg - k_vec.dot(term1))\n",
    "        c = max(c, 1e-9)\n",
    "\n",
    "        n_old = len(X_arm) - 1\n",
    "        K_inv_new = np.zeros((n_old + 1, n_old + 1), dtype=float)\n",
    "\n",
    "        K_inv_new[:n_old, :n_old] = K_inv + np.outer(term1, term1) / c\n",
    "        v = -term1 / c\n",
    "        K_inv_new[:n_old, n_old] = v\n",
    "        K_inv_new[n_old, :n_old] = v\n",
    "        K_inv_new[n_old, n_old] = 1.0 / c\n",
    "\n",
    "        self.K_inv[arm] = K_inv_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3058d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(dataset=None,fusion=None,algo='linucb'):\n",
    "    # Configuration\n",
    "\n",
    "\n",
    "    DATA_PATH = f\"data/bandit_data_train_{dataset}_{fusion}.jsonl\"\n",
    "    OUTPUT_PATH = f\"data/{algo}_training_history_{dataset}_{fusion}.jsonl\"\n",
    "    N_ARMS = 5\n",
    "    N_FEATURES = 5\n",
    "    ALPHA = 1  # Exploration parameter\n",
    "    TOTAL_STEPS = 50000  # Adjust based on dataset size\n",
    "\n",
    "    print(f\"Initializing Environment from {DATA_PATH}...\")\n",
    "    try:\n",
    "        dataset = []\n",
    "        with open(DATA_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                # add 'dataset_name' field\n",
    "                data_entry = json.loads(line)\n",
    "                data_entry['dataset_name'] = \"bandit_data_train\"\n",
    "                dataset.append(data_entry)\n",
    "        train_env = Environment(arms=N_ARMS, dataset=dataset)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {DATA_PATH}. Please run Phase 1 (Data Generation) first.\")\n",
    "        return\n",
    "\n",
    "    if algo == 'linucb':\n",
    "        print(f\"Initializing LinUCB Agent (Arms={N_ARMS}, Features={N_FEATURES}, Alpha={ALPHA})...\")\n",
    "        agent = LinUCBAgent(n_arms=N_ARMS, n_features=N_FEATURES, alpha=ALPHA)\n",
    "    elif algo == 'fku':\n",
    "        print(f\"Initializing FastKernelUCB Agent (Arms={N_ARMS}, Alpha={ALPHA})...\")\n",
    "        agent = FastKernelUCBAgent(n_arms=N_ARMS, alpha=ALPHA, device='cpu')\n",
    "    else:\n",
    "        print(f\"Error: Unknown algorithm '{algo}'. Supported: 'linucb', 'fku'.\")\n",
    "        return\n",
    "\n",
    "    history = []\n",
    "    cumulative_reward = 0.0\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # tqdm provides a progress bar\n",
    "    for step in tqdm(range(TOTAL_STEPS), desc=\"Training\"):\n",
    "        # 1. Get Context\n",
    "        # The Environment cycles through the pre-computed dataset\n",
    "        query_data = train_env.dataset[train_env.index]\n",
    "        context = np.array(query_data.get('features', np.random.rand(N_FEATURES)))\n",
    "        \n",
    "        # 2. Select Action (Bandit Decision)\n",
    "        chosen_arm = agent.select_arm(context)\n",
    "        \n",
    "        # 3. Get Reward (Simulate Partial Feedback)\n",
    "        # We only reveal the reward for the arm we actually picked\n",
    "        # print(\"Chosen Arm:\", chosen_arm)\n",
    "        reward = train_env.choose_arm(chosen_arm)\n",
    "        \n",
    "        # 4. Update Policy\n",
    "        agent.update(chosen_arm, context, reward)\n",
    "        \n",
    "        # 5. Logging\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        log_entry = {\n",
    "            'step': step,\n",
    "            'query_id': query_data.get('query_id', 'unknown'),\n",
    "            'chosen_arm': int(chosen_arm),\n",
    "            'reward': float(reward),\n",
    "            'optimal_arm': int(query_data.get('optimal_arm', -1)),\n",
    "            'regret': float(query_data['rewards'][query_data['optimal_arm']] - reward),\n",
    "            'cumulative_reward': cumulative_reward\n",
    "        }\n",
    "        history.append(log_entry)\n",
    "\n",
    "    # Save training history for analysis (Plotting Regret/Arm Distribution)\n",
    "    print(f\"Saving training history to {OUTPUT_PATH}...\")\n",
    "    with open(OUTPUT_PATH, 'w') as f:\n",
    "        for entry in history:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "720f20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Environment from data/bandit_data_train_msmarco_minmax.jsonl...\n",
      "Initializing FastKernelUCB Agent (Arms=5, Alpha=1)...\n",
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 8548/50000 [02:16<11:03, 62.50it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train_agent(dataset='msmarco',fusion='minmax',algo='linucb')\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# train_agent(dataset='msmarco', fusion='rrf', algo='linucb')\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# train_agent(dataset='msmarco', fusion='zscore', algo='linucb')\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmsmarco\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfusion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mminmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfku\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(dataset, fusion, algo)\u001b[39m\n\u001b[32m     53\u001b[39m reward = train_env.choose_arm(chosen_arm)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 4. Update Policy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchosen_arm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# 5. Logging\u001b[39;00m\n\u001b[32m     59\u001b[39m cumulative_reward += reward\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mFastKernelUCBAgent.update\u001b[39m\u001b[34m(self, arm, x_new, reward)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Compute quantities for block matrix inverse update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m k_vec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kernel_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_arm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (n_old,)\u001b[39;00m\n\u001b[32m     70\u001b[39m k_xx = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.kernel_fn(x, x, sigma=\u001b[38;5;28mself\u001b[39m.sigma))\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Ensure K_inv is np.ndarray\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mFastKernelUCBAgent._kernel_vector\u001b[39m\u001b[34m(self, x, X_list)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_kernel_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, X_list):\n\u001b[32m     25\u001b[39m     x = np.asarray(x, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X_list], dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrbf_kernel\u001b[39m\u001b[34m(x, y, sigma)\u001b[39m\n\u001b[32m      2\u001b[39m x = np.asarray(x, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m      3\u001b[39m y = np.asarray(y, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train_agent(dataset='msmarco',fusion='minmax',algo='linucb')\n",
    "# train_agent(dataset='msmarco', fusion='rrf', algo='linucb')\n",
    "# train_agent(dataset='msmarco', fusion='zscore', algo='linucb')\n",
    "train_agent(dataset='msmarco',fusion='minmax',algo='fku')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c263d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1a4267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training history and dataset...\n",
      "\n",
      "=== Training Analysis ===\n",
      "Total steps: 50000\n",
      "Dataset queries: 50000\n",
      "\n",
      "Reward mismatches: 0\n",
      "Optimal arm mismatches: 0\n",
      "\n",
      "=== Regret Statistics ===\n",
      "Mean regret: 0.089998\n",
      "Min regret: 0.000000\n",
      "Max regret: 1.000000\n",
      "Total cumulative regret: 4499.887314\n",
      "\n",
      "=== Arm Selection Distribution ===\n",
      "Arm 0: 6101 times (12.20%)\n",
      "Arm 1: 30889 times (61.78%)\n",
      "Arm 2: 12044 times (24.09%)\n",
      "Arm 3: 684 times (1.37%)\n",
      "Arm 4: 282 times (0.56%)\n",
      "\n",
      "=== Reward Trend ===\n",
      "Final cumulative reward: 19520.384885\n",
      "Average reward per step: 0.390408\n",
      "Non-monotonic steps (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_results():\n",
    "    \"\"\"Verify training results against original dataset.\"\"\"\n",
    "    HISTORY_PATH = \"../MABhybrid/data/linucb_training_history.jsonl\"\n",
    "    DATA_PATH = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "    \n",
    "    print(\"Loading training history and dataset...\")\n",
    "    history = []\n",
    "    with open(HISTORY_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            history.append(json.loads(line))\n",
    "    \n",
    "    dataset = {}\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            dataset[entry['query_id']] = entry\n",
    "    \n",
    "    print(f\"\\n=== Training Analysis ===\")\n",
    "    print(f\"Total steps: {len(history)}\")\n",
    "    print(f\"Dataset queries: {len(dataset)}\")\n",
    "    \n",
    "    # Check 1: Verify rewards match dataset\n",
    "    mismatches = 0\n",
    "    for log in history[:100]:  # Check first 100\n",
    "        qid = log['query_id']\n",
    "        arm = log['chosen_arm']\n",
    "        logged_reward = log['reward']\n",
    "        \n",
    "        if qid in dataset:\n",
    "            expected_reward = dataset[qid]['rewards'][arm]\n",
    "            if abs(logged_reward - expected_reward) > 1e-6:\n",
    "                print(f\"MISMATCH Step {log['step']}: Query {qid}, Arm {arm}\")\n",
    "                print(f\"  Logged: {logged_reward}, Expected: {expected_reward}\")\n",
    "                mismatches += 1\n",
    "    \n",
    "    print(f\"\\nReward mismatches: {mismatches}\")\n",
    "    \n",
    "    # Check 2: Verify optimal_arm matches\n",
    "    optimal_mismatches = 0\n",
    "    for log in history[:100]:\n",
    "        qid = log['query_id']\n",
    "        if qid in dataset:\n",
    "            logged_optimal = log['optimal_arm']\n",
    "            expected_optimal = dataset[qid]['optimal_arm']\n",
    "            if logged_optimal != expected_optimal:\n",
    "                print(f\"OPTIMAL MISMATCH Step {log['step']}: Query {qid}\")\n",
    "                print(f\"  Logged: {logged_optimal}, Expected: {expected_optimal}\")\n",
    "                optimal_mismatches += 1\n",
    "    \n",
    "    print(f\"Optimal arm mismatches: {optimal_mismatches}\")\n",
    "    \n",
    "    # Check 3: Regret calculation\n",
    "    print(f\"\\n=== Regret Statistics ===\")\n",
    "    regrets = [log['regret'] for log in history]\n",
    "    print(f\"Mean regret: {np.mean(regrets):.6f}\")\n",
    "    print(f\"Min regret: {np.min(regrets):.6f}\")\n",
    "    print(f\"Max regret: {np.max(regrets):.6f}\")\n",
    "    print(f\"Total cumulative regret: {sum(regrets):.6f}\")\n",
    "    \n",
    "    # Check 4: Arm distribution\n",
    "    print(f\"\\n=== Arm Selection Distribution ===\")\n",
    "    arm_counts = {}\n",
    "    for log in history:\n",
    "        arm = log['chosen_arm']\n",
    "        arm_counts[arm] = arm_counts.get(arm, 0) + 1\n",
    "    \n",
    "    for arm in sorted(arm_counts.keys()):\n",
    "        pct = 100 * arm_counts[arm] / len(history)\n",
    "        print(f\"Arm {arm}: {arm_counts[arm]} times ({pct:.2f}%)\")\n",
    "    \n",
    "    # Check 5: Cumulative reward trend\n",
    "    print(f\"\\n=== Reward Trend ===\")\n",
    "    final_cumulative = history[-1]['cumulative_reward']\n",
    "    print(f\"Final cumulative reward: {final_cumulative:.6f}\")\n",
    "    print(f\"Average reward per step: {final_cumulative / len(history):.6f}\")\n",
    "    \n",
    "    # Check monotonicity\n",
    "    non_monotonic = 0\n",
    "    for i in range(1, len(history)):\n",
    "        if history[i]['cumulative_reward'] < history[i-1]['cumulative_reward']:\n",
    "            non_monotonic += 1\n",
    "    print(f\"Non-monotonic steps (should be 0): {non_monotonic}\")\n",
    "\n",
    "analyze_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bba72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training history...\n",
      "Loading ground truth data for baselines...\n",
      "Loaded 50000 training steps.\n",
      "Saved Comparative Reward plot to ../MABhybrid/fig/comparative_reward.png\n",
      "Saved Comparative Regret plot to ../MABhybrid/fig/comparative_regret.png\n",
      "\n",
      "--- Reconstructing Agent for Feature Analysis ---\n",
      "\n",
      "[Interpretability Result] Learned Feature Weights:\n",
      "\n",
      "Arm 0 (Dense Retrieval):\n",
      "  Length    : -0.0134\n",
      "  MaxIDF    : -0.0068\n",
      "  AvgIDF    :  0.0102\n",
      "  QFlag     :  0.0103\n",
      "  Bias      :  0.4463\n",
      "\n",
      "Arm 4 (Sparse Retrieval):\n",
      "  Length    :  0.0157\n",
      "  MaxIDF    : -0.0033\n",
      "  AvgIDF    :  0.0212\n",
      "  QFlag     : -0.1010\n",
      "  Bias      : -0.0785\n",
      "\n",
      "Analysis complete. Check ../MABhybrid/fig for plots.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "HISTORY_FILE = \"../MABhybrid/data/linucb_training_history.jsonl\"\n",
    "DATA_FILE = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "OUTPUT_DIR = \"../MABhybrid/fig\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads history and enriches it with baseline rewards from ground truth.\"\"\"\n",
    "    if not os.path.exists(HISTORY_FILE) or not os.path.exists(DATA_FILE):\n",
    "        print(\"Error: Missing history or data file.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Loading training history...\")\n",
    "    history = []\n",
    "    with open(HISTORY_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            history.append(json.loads(line))\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "    print(\"Loading ground truth data for baselines...\")\n",
    "    ground_truth = []\n",
    "    with open(DATA_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            ground_truth.append(json.loads(line))\n",
    "    \n",
    "    # Calculate Baseline Rewards\n",
    "    # We assume the training loop iterated through ground_truth sequentially\n",
    "    n_data = len(ground_truth)\n",
    "    \n",
    "    static_rewards = []\n",
    "    random_rewards = []\n",
    "    optimal_rewards = []\n",
    "\n",
    "    for step in df['step']:\n",
    "        # Map step to index in the original data file (handling wrap-around if any)\n",
    "        idx = step % n_data\n",
    "        entry = ground_truth[idx]\n",
    "        \n",
    "        # Static Baseline: Always choose Arm 2 (Index 2 corresponds to alpha=0.5)\n",
    "        static_rewards.append(entry['rewards'][2])\n",
    "        \n",
    "        # Random Baseline: Expected value is the mean of all arms\n",
    "        random_rewards.append(np.mean(entry['rewards']))\n",
    "        \n",
    "        # Optimal Reward (for calculating regret)\n",
    "        optimal_rewards.append(max(entry['rewards']))\n",
    "\n",
    "    df['static_reward'] = static_rewards\n",
    "    df['random_reward'] = random_rewards\n",
    "    df['optimal_reward'] = optimal_rewards\n",
    "    \n",
    "    # Calculate Regrets\n",
    "    df['static_regret'] = df['optimal_reward'] - df['static_reward']\n",
    "    df['random_regret'] = df['optimal_reward'] - df['random_reward']\n",
    "    # 'regret' column already exists for the agent in the history file\n",
    "    \n",
    "    return df, ground_truth\n",
    "\n",
    "def plot_comparative_cumulative_reward(df):\n",
    "    \"\"\"\n",
    "    Plots Cumulative Mean Reward: Agent vs Static vs Random\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate expanding means (cumulative average)\n",
    "    df['agent_cum_mean'] = df['reward'].expanding().mean()\n",
    "    df['static_cum_mean'] = df['static_reward'].expanding().mean()\n",
    "    df['random_cum_mean'] = df['random_reward'].expanding().mean()\n",
    "    \n",
    "    plt.plot(df['step'], df['agent_cum_mean'], label='LinUCB Agent', color='blue', linewidth=2)\n",
    "    plt.plot(df['step'], df['static_cum_mean'], label='Static (α=0.5)', color='green', linestyle='--')\n",
    "    plt.plot(df['step'], df['random_cum_mean'], label='Random', color='gray', linestyle=':')\n",
    "    \n",
    "    plt.title('Performance Comparison: Cumulative Mean Reward (NDCG@10)')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Average Reward (NDCG)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, 'comparative_reward.png')\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Saved Comparative Reward plot to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_comparative_cumulative_regret(df):\n",
    "    \"\"\"\n",
    "    Plots Cumulative Regret: Agent vs Static vs Random\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(df['step'], df['regret'].cumsum(), label='LinUCB Agent', color='red', linewidth=2)\n",
    "    plt.plot(df['step'], df['static_regret'].cumsum(), label='Static (α=0.5)', color='green', linestyle='--')\n",
    "    plt.plot(df['step'], df['random_regret'].cumsum(), label='Random', color='gray', linestyle=':')\n",
    "    \n",
    "    plt.title('Performance Comparison: Cumulative Regret')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Cumulative Regret (Lost NDCG)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, 'comparative_regret.png')\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Saved Comparative Regret plot to {output_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_arm_distribution(df):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    arm_counts = df['chosen_arm'].value_counts().sort_index()\n",
    "    arms = arm_counts.index.tolist()\n",
    "    counts = arm_counts.values.tolist()\n",
    "    alpha_map = {0: '0.0 (Dense)', 1: '0.25', 2: '0.5 (Hybrid)', 3: '0.75', 4: '1.0 (Sparse)'}\n",
    "    labels = [alpha_map.get(a, str(a)) for a in arms]\n",
    "    \n",
    "    plt.bar(arms, counts, color='purple', alpha=0.7)\n",
    "    plt.xticks(arms, labels, rotation=45)\n",
    "    plt.title('Agent Choice Distribution')\n",
    "    plt.xlabel('Arm (Alpha Value)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'arm_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_importance(history_df, ground_truth_data):\n",
    "    \"\"\"\n",
    "    Reconstructs the agent to analyze feature weights.\n",
    "    \"\"\"\n",
    "    # Import locally to avoid issues if MABhybrid isn't in pythonpath\n",
    "    # sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'MABhybrid'))\n",
    "    # try:\n",
    "    #     from linucb import LinUCBAgent\n",
    "    # except ImportError:\n",
    "    #     print(\"Could not import LinUCBAgent for feature analysis.\")\n",
    "    #     return\n",
    "\n",
    "    print(\"\\n--- Reconstructing Agent for Feature Analysis ---\")\n",
    "    # 5 features: Length, MaxIDF, AvgIDF, QFlag, Bias\n",
    "    agent = LinUCBAgent(n_arms=5, n_features=5, alpha=0.1)\n",
    "    \n",
    "    # Replay history to update agent\n",
    "    n_data = len(ground_truth_data)\n",
    "    \n",
    "    # Limit reconstruction to first 50k steps or full length to save time if large\n",
    "    steps_to_replay = min(len(history_df), 50000) \n",
    "    \n",
    "    for i in range(steps_to_replay):\n",
    "        row = history_df.iloc[i]\n",
    "        step_idx = int(row['step']) % n_data\n",
    "        \n",
    "        # Get context from ground truth\n",
    "        features = np.array(ground_truth_data[step_idx]['features'])\n",
    "        chosen_arm = int(row['chosen_arm'])\n",
    "        reward = float(row['reward'])\n",
    "        \n",
    "        agent.update(chosen_arm, features, reward)\n",
    "\n",
    "    feature_names = ['Length', 'MaxIDF', 'AvgIDF', 'QFlag', 'Bias']\n",
    "    \n",
    "    # Print weights for Sparse (Arm 4) vs Dense (Arm 0)\n",
    "    print(\"\\n[Interpretability Result] Learned Feature Weights:\")\n",
    "    \n",
    "    # Helper to print vector\n",
    "    def print_arm_weights(arm_idx, name):\n",
    "        theta = np.linalg.inv(agent.A[arm_idx]) @ agent.b[arm_idx]\n",
    "        print(f\"\\nArm {arm_idx} ({name}):\")\n",
    "        for f, w in zip(feature_names, theta):\n",
    "            print(f\"  {f:10s}: {w: .4f}\")\n",
    "\n",
    "    print_arm_weights(0, \"Dense Retrieval\")\n",
    "    print_arm_weights(4, \"Sparse Retrieval\")\n",
    "\n",
    "def main():\n",
    "    result = load_data()\n",
    "    if result is None: return\n",
    "    df, ground_truth = result\n",
    "    \n",
    "    print(f\"Loaded {len(df)} training steps.\")\n",
    "    \n",
    "    plot_comparative_cumulative_reward(df)\n",
    "    plot_comparative_cumulative_regret(df)\n",
    "    plot_arm_distribution(df)\n",
    "    analyze_feature_importance(df, ground_truth)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Check {OUTPUT_DIR} for plots.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
