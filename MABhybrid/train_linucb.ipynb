{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc51a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a864f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAB utils\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "# from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import re\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import pandas as pd\n",
    "import ast  \n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, arms, dataset, args=None, preding=False):\n",
    "        self.arms = arms\n",
    "        self.dataset = dataset\n",
    "        self.preding = preding\n",
    "        self.index = -1\n",
    "        self.alpha_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        self.skip_dataset = []\n",
    "        self._update_state()\n",
    "        \n",
    "\n",
    "    def _update_state(self):\n",
    "        self.index += 1\n",
    "        if self.index >= len(self.dataset):\n",
    "            self.index = 0\n",
    "        \n",
    "        while self.dataset[self.index]['dataset_name'] in self.skip_dataset and not self.preding:\n",
    "            self.index += 1\n",
    "            if self.index >= len(self.dataset):\n",
    "                self.index = 0\n",
    "\n",
    "        self.state = self.dataset[self.index]['text']\n",
    "        \n",
    "        # self.state = np.random.randint(0, self.arms)\n",
    "    def _index_to_arm(self,index):\n",
    "        if type(index) == np.ndarray:\n",
    "            assert len(index) == 1\n",
    "            index = index[0]\n",
    "        return self.alpha_values[int(index)]\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "        # return self.state\n",
    "\n",
    "    def _get_reward(self, arm):\n",
    "        \"\"\"\n",
    "        Returns the pre-computed reward for the selected arm.\n",
    "        arm (int): Index 0-4 corresponding to alpha values [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "        \"\"\"\n",
    "        query_data = self.dataset[self.index]\n",
    "        rewards = query_data.get(\"rewards\", [0.0] * 5)\n",
    "        \n",
    "        # Validate arm index\n",
    "        if arm < 0 or arm >= len(rewards):\n",
    "            print(f\"Warning: Arm {arm} out of range for rewards list of length {len(rewards)}\")\n",
    "            return 0.0\n",
    "        \n",
    "        return float(rewards[int(arm)])\n",
    "                               \n",
    "\n",
    "    def _get_recall(self,arm):\n",
    "        raise NotImplementedError\n",
    "        method = self._index_to_arm(arm)\n",
    "        return self.dataset[self.index][method+'_eval']['recall']\n",
    "\n",
    "    def choose_arm(self, arm):\n",
    "        reward = self._get_reward(arm)\n",
    "        # recall = self._get_recall(arm)\n",
    "        self._update_state()\n",
    "        return reward\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1250827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinUCBAgent:\n",
    "    \"\"\"\n",
    "    Disjoint Linear Upper Confidence Bound (LinUCB) Agent.\n",
    "    \n",
    "    References:\n",
    "        Li et al., \"A Contextual-Bandit Approach to Personalized News Article Recommendation\", WWW 2010.\n",
    "        (Algorithm 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, n_features, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_arms (int): Number of distinct actions (fusion weights).\n",
    "            n_features (int): Dimension of the context vector.\n",
    "            alpha (float): Exploration hyperparameter. Higher alpha = more exploration.\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize disjoint matrices for each arm\n",
    "        # A: Covariance matrix (d x d), initialized to Identity for Ridge Regularization\n",
    "        # b: Reward-weighted feature vector (d x 1), initialized to zeros\n",
    "        self.A = [np.identity(n_features) for _ in range(n_arms)]\n",
    "        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self, context_vector):\n",
    "        \"\"\"\n",
    "        Selects an arm based on the Upper Confidence Bound (UCB) of the estimated reward.\n",
    "        \n",
    "        Args:\n",
    "            context_vector (np.array): Shape (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of the selected arm.\n",
    "        \"\"\"\n",
    "        p = np.zeros(self.n_arms)\n",
    "        \n",
    "        for a in range(self.n_arms):\n",
    "            # 1. Compute the inverse of A (Ridge Regression covariance)\n",
    "            # In production, use np.linalg.solve or update inverse iteratively for speed\n",
    "            A_inv = np.linalg.inv(self.A[a])\n",
    "            \n",
    "            # 2. Compute the estimated coefficient (theta)\n",
    "            # theta = A^-1 * b\n",
    "            theta = A_inv @ self.b[a]\n",
    "            \n",
    "            # 3. Calculate the standard deviation (uncertainty width)\n",
    "            # std = sqrt(x.T * A^-1 * x)\n",
    "            std_dev = np.sqrt(context_vector.T @ A_inv @ context_vector)\n",
    "            \n",
    "            # 4. Calculate UCB\n",
    "            # Prediction + Exploration Bonus\n",
    "            p[a] = theta @ context_vector + self.alpha * std_dev\n",
    "            \n",
    "        # Tie-breaking: randomly choose among max if multiple arms share the same score\n",
    "        # (np.argmax usually takes the first occurrence, which is fine here)\n",
    "        return np.argmax(p)\n",
    "\n",
    "    def update(self, arm, context_vector, reward):\n",
    "        \"\"\"\n",
    "        Updates the internal matrices A and b for the specific arm that was chosen.\n",
    "        \n",
    "        Args:\n",
    "            arm (int): The arm index that was selected.\n",
    "            context_vector (np.array): The feature vector observed.\n",
    "            reward (float): The actual reward (NDCG) received.\n",
    "        \"\"\"\n",
    "        # Outer product of context vector (d x d)\n",
    "        self.A[arm] += np.outer(context_vector, context_vector)\n",
    "        \n",
    "        # Update bias vector\n",
    "        self.b[arm] += reward * context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3058d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "    OUTPUT_PATH = \"../MABhybrid/data/linucb_training_history.jsonl\"\n",
    "    N_ARMS = 5\n",
    "    N_FEATURES = 5\n",
    "    ALPHA = 1  # Exploration parameter\n",
    "    TOTAL_STEPS = 50000  # Adjust based on dataset size\n",
    "\n",
    "    print(f\"Initializing Environment from {DATA_PATH}...\")\n",
    "    try:\n",
    "        dataset = []\n",
    "        with open(DATA_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                # add 'dataset_name' field\n",
    "                data_entry = json.loads(line)\n",
    "                data_entry['dataset_name'] = \"bandit_data_train\"\n",
    "                dataset.append(data_entry)\n",
    "        train_env = Environment(arms=N_ARMS, dataset=dataset)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {DATA_PATH}. Please run Phase 1 (Data Generation) first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Initializing LinUCB Agent (Arms={N_ARMS}, Features={N_FEATURES}, Alpha={ALPHA})...\")\n",
    "    agent = LinUCBAgent(n_arms=N_ARMS, n_features=N_FEATURES, alpha=ALPHA)\n",
    "\n",
    "    history = []\n",
    "    cumulative_reward = 0.0\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # tqdm provides a progress bar\n",
    "    for step in tqdm(range(TOTAL_STEPS), desc=\"Training\"):\n",
    "        # 1. Get Context\n",
    "        # The Environment cycles through the pre-computed dataset\n",
    "        query_data = train_env.dataset[train_env.index]\n",
    "        context = np.array(query_data.get('features', np.random.rand(N_FEATURES)))\n",
    "        \n",
    "        # 2. Select Action (Bandit Decision)\n",
    "        chosen_arm = agent.select_arm(context)\n",
    "        \n",
    "        # 3. Get Reward (Simulate Partial Feedback)\n",
    "        # We only reveal the reward for the arm we actually picked\n",
    "        # print(\"Chosen Arm:\", chosen_arm)\n",
    "        reward = train_env.choose_arm(chosen_arm)\n",
    "        \n",
    "        # 4. Update Policy\n",
    "        agent.update(chosen_arm, context, reward)\n",
    "        \n",
    "        # 5. Logging\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        log_entry = {\n",
    "            'step': step,\n",
    "            'query_id': query_data.get('query_id', 'unknown'),\n",
    "            'chosen_arm': int(chosen_arm),\n",
    "            'reward': float(reward),\n",
    "            'optimal_arm': int(query_data.get('optimal_arm', -1)),\n",
    "            'regret': float(query_data['rewards'][query_data['optimal_arm']] - reward),\n",
    "            'cumulative_reward': cumulative_reward\n",
    "        }\n",
    "        history.append(log_entry)\n",
    "\n",
    "    # Save training history for analysis (Plotting Regret/Arm Distribution)\n",
    "    print(f\"Saving training history to {OUTPUT_PATH}...\")\n",
    "    with open(OUTPUT_PATH, 'w') as f:\n",
    "        for entry in history:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "720f20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Environment from ../MABhybrid/data/bandit_data_train.jsonl...\n",
      "Initializing LinUCB Agent (Arms=5, Features=5, Alpha=1)...\n",
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50000/50000 [00:01<00:00, 29441.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training history to ../MABhybrid/data/linucb_training_history.jsonl...\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1a4267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training history and dataset...\n",
      "\n",
      "=== Training Analysis ===\n",
      "Total steps: 50000\n",
      "Dataset queries: 50000\n",
      "\n",
      "Reward mismatches: 0\n",
      "Optimal arm mismatches: 0\n",
      "\n",
      "=== Regret Statistics ===\n",
      "Mean regret: 0.089998\n",
      "Min regret: 0.000000\n",
      "Max regret: 1.000000\n",
      "Total cumulative regret: 4499.887314\n",
      "\n",
      "=== Arm Selection Distribution ===\n",
      "Arm 0: 6101 times (12.20%)\n",
      "Arm 1: 30889 times (61.78%)\n",
      "Arm 2: 12044 times (24.09%)\n",
      "Arm 3: 684 times (1.37%)\n",
      "Arm 4: 282 times (0.56%)\n",
      "\n",
      "=== Reward Trend ===\n",
      "Final cumulative reward: 19520.384885\n",
      "Average reward per step: 0.390408\n",
      "Non-monotonic steps (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "def analyze_training_results():\n",
    "    \"\"\"Verify training results against original dataset.\"\"\"\n",
    "    HISTORY_PATH = \"../MABhybrid/data/linucb_training_history.jsonl\"\n",
    "    DATA_PATH = \"../MABhybrid/data/bandit_data_train.jsonl\"\n",
    "    \n",
    "    print(\"Loading training history and dataset...\")\n",
    "    history = []\n",
    "    with open(HISTORY_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            history.append(json.loads(line))\n",
    "    \n",
    "    dataset = {}\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            dataset[entry['query_id']] = entry\n",
    "    \n",
    "    print(f\"\\n=== Training Analysis ===\")\n",
    "    print(f\"Total steps: {len(history)}\")\n",
    "    print(f\"Dataset queries: {len(dataset)}\")\n",
    "    \n",
    "    # Check 1: Verify rewards match dataset\n",
    "    mismatches = 0\n",
    "    for log in history[:100]:  # Check first 100\n",
    "        qid = log['query_id']\n",
    "        arm = log['chosen_arm']\n",
    "        logged_reward = log['reward']\n",
    "        \n",
    "        if qid in dataset:\n",
    "            expected_reward = dataset[qid]['rewards'][arm]\n",
    "            if abs(logged_reward - expected_reward) > 1e-6:\n",
    "                print(f\"MISMATCH Step {log['step']}: Query {qid}, Arm {arm}\")\n",
    "                print(f\"  Logged: {logged_reward}, Expected: {expected_reward}\")\n",
    "                mismatches += 1\n",
    "    \n",
    "    print(f\"\\nReward mismatches: {mismatches}\")\n",
    "    \n",
    "    # Check 2: Verify optimal_arm matches\n",
    "    optimal_mismatches = 0\n",
    "    for log in history[:100]:\n",
    "        qid = log['query_id']\n",
    "        if qid in dataset:\n",
    "            logged_optimal = log['optimal_arm']\n",
    "            expected_optimal = dataset[qid]['optimal_arm']\n",
    "            if logged_optimal != expected_optimal:\n",
    "                print(f\"OPTIMAL MISMATCH Step {log['step']}: Query {qid}\")\n",
    "                print(f\"  Logged: {logged_optimal}, Expected: {expected_optimal}\")\n",
    "                optimal_mismatches += 1\n",
    "    \n",
    "    print(f\"Optimal arm mismatches: {optimal_mismatches}\")\n",
    "    \n",
    "    # Check 3: Regret calculation\n",
    "    print(f\"\\n=== Regret Statistics ===\")\n",
    "    regrets = [log['regret'] for log in history]\n",
    "    print(f\"Mean regret: {np.mean(regrets):.6f}\")\n",
    "    print(f\"Min regret: {np.min(regrets):.6f}\")\n",
    "    print(f\"Max regret: {np.max(regrets):.6f}\")\n",
    "    print(f\"Total cumulative regret: {sum(regrets):.6f}\")\n",
    "    \n",
    "    # Check 4: Arm distribution\n",
    "    print(f\"\\n=== Arm Selection Distribution ===\")\n",
    "    arm_counts = {}\n",
    "    for log in history:\n",
    "        arm = log['chosen_arm']\n",
    "        arm_counts[arm] = arm_counts.get(arm, 0) + 1\n",
    "    \n",
    "    for arm in sorted(arm_counts.keys()):\n",
    "        pct = 100 * arm_counts[arm] / len(history)\n",
    "        print(f\"Arm {arm}: {arm_counts[arm]} times ({pct:.2f}%)\")\n",
    "    \n",
    "    # Check 5: Cumulative reward trend\n",
    "    print(f\"\\n=== Reward Trend ===\")\n",
    "    final_cumulative = history[-1]['cumulative_reward']\n",
    "    print(f\"Final cumulative reward: {final_cumulative:.6f}\")\n",
    "    print(f\"Average reward per step: {final_cumulative / len(history):.6f}\")\n",
    "    \n",
    "    # Check monotonicity\n",
    "    non_monotonic = 0\n",
    "    for i in range(1, len(history)):\n",
    "        if history[i]['cumulative_reward'] < history[i-1]['cumulative_reward']:\n",
    "            non_monotonic += 1\n",
    "    print(f\"Non-monotonic steps (should be 0): {non_monotonic}\")\n",
    "\n",
    "analyze_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bba72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
